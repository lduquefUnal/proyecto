{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a6eebc8",
   "metadata": {},
   "source": [
    "# Redes neuronales hibridas  para clasificación multiple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1f1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a63f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the relevant packages.\n",
    "#%pip install --upgrade pip\n",
    "#%pip install torch torchvision torchaudio\n",
    "#%pip install cudaq -> ya viene instalado en el braket de AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50673393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check installed clasico\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
    "\n",
    "import torch, torchvision, torchaudio\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"vision:\", torchvision.__version__)\n",
    "print(\"audio:\", torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cudaq\n",
    "print(f\"CUDAQ version: {cudaq.__version__}\")\n",
    "print(f\"Running on target: {cudaq.get_target().name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e02ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cudaq\n",
    "\n",
    "print(f\"Running on target {cudaq.get_target().name}\")\n",
    "qubit_count = 2\n",
    "\n",
    "\n",
    "@cudaq.kernel\n",
    "def kernel():\n",
    "    qubits = cudaq.qvector(qubit_count)\n",
    "    h(qubits[0])\n",
    "    for i in range(1, qubit_count):\n",
    "        x.ctrl(qubits[0], qubits[i])\n",
    "    mz(qubits)\n",
    "\n",
    "\n",
    "result = cudaq.sample(kernel)\n",
    "print(result)  # Example: { 11:500 00:500 }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d874d9c8-a2b1-4553-b0d5-98d1bfb04ae3",
   "metadata": {},
   "source": [
    "### Configuración de Hiperparámetros\n",
    "\n",
    "Para facilitar la experimentación, todos los hiperparámetros importantes se definen en la siguiente celda. Puedes modificar estos valores para ver cómo afectan el rendimiento y el tiempo de entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd250776-108f-4571-8003-b00e008be7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hiperparámetros del Modelo y Entrenamiento ---\n",
    "# Versión ligera: 4 qubits + medición AMM (12 features) + PQC en escalera poco profunda\n",
    "n_qubits = 4\n",
    "pqc_layers = 2  # profundidad baja para reducir llamadas cuánticas\n",
    "n_samples_prueba = 2000  # subconjunto para experimentar rápido; pon 60000 para todo MNIST reducido\n",
    "batch_size = 16\n",
    "epochs = 15\n",
    "learning_rate = 8e-2  # LR moderado para estabilidad con el bloque cuántico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37087a4",
   "metadata": {},
   "source": [
    "### Estructura del modelo AMM ligero\n",
    "- Entrada: imagen MNIST reducida 4×4 (16 px) -> encoder clásico `Flatten -> Linear(16→32) -> ReLU -> Linear(32→4)`; ángulos $\\alpha = \\arcsin(\\sigma(h)) \\in \\mathbb{R}^4$ con $\\sigma$ = sigmoid.\n",
    "- PQC: escalera de profundidad $L=2$ con $n_q=4$ qubits y compuertas ZZ (descompuestas en CX-RZ-CX). Estado final $U_{\\text{PQC}}(\\theta)\\, U_{\\text{enc}}(\\alpha)\\, |0\\rangle^{\\otimes n_q}$.\n",
    "- Medición AMM: se miden $\\langle \\sigma_x \\rangle, \\langle \\sigma_y \\rangle, \\langle \\sigma_z \\rangle$ en cada qubit; $m_{\\text{AMM}} \\in \\mathbb{R}^{3 n_q}$ (12 features para 4 qubits).\n",
    "- Clasificador: `LayerNorm -> Linear(12→32) -> ReLU -> Linear(32→10)`; `CrossEntropyLoss` aplica softmax interno.\n",
    "\n",
    "### Flujo de datos y etiquetado\n",
    "- Etiquetas $y \\in \\{0,\\dots,9\\}$ provienen directamente de MNIST (sin one-hot).\n",
    "- Forward: $z = f_{\\text{class}}(m_{\\text{AMM}})$; pred = argmax$(z)$; $\\mathcal{L} = \\text{CE}(z, y)$. Misma división train/val/test que en el resto del notebook.\n",
    "\n",
    "### Diferencias con `red_hibrida.ipynb`\n",
    "- Observables: allí se mide un solo observable ($\\sum Z$ -> 1 feature); aquí AMM en 4 qubits entrega 12 features, más señal para el clasificador.\n",
    "- Encoder: allí CNN completa 28×28 + FC grande; aquí reducción 4×4 + encoder pequeño, bajando cómputo y llamadas cuánticas.\n",
    "- Gradientes: allí se propagan gradientes también a las entradas del PQC; aquí solo a $\\theta$ del PQC (parameter-shift), lo que reduce ejecuciones cuánticas en backward.\n",
    "\n",
    "### Optimización y parameter-shift\n",
    "- Pérdida en un minibatch $B$: $\\mathcal{L} = -\\frac{1}{|B|} \\sum_{i \\in B} \\log(\\text{softmax}(z_i)_{y_i})$.\n",
    "- Gradientes cuánticos via parameter-shift para cada $\\theta_j$: $\\partial_{\\theta_j}\\mathcal{L} \\approx \\tfrac{1}{2} [\\mathcal{L}(\\theta_j + \\tfrac{\\pi}{2}) - \\mathcal{L}(\\theta_j - \\tfrac{\\pi}{2})]$, aplicados solo a las $\\theta$ del PQC; encoder y clasificador usan backprop estándar.\n",
    "- Actualización con Adam (lr = $8\\times10^{-4}$) y scheduler StepLR ($\\gamma=0.2$ cada 7 épocas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918cbda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar las librerías necesarias\n",
    "import cudaq\n",
    "from cudaq import spin\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Para asegurar que los resultados sean reproducibles\n",
    "torch.manual_seed(33)\n",
    "\n",
    "cudaq.set_random_seed(33)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582043a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el dispositivo\n",
    "# Set CUDAQ and PyTorch to run on either CPU or GPU.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    cudaq.set_target(\"nvidia\")\n",
    "    print(\"✅ Dispositivo configurado para GPU (CUDA).\")\n",
    "else:\n",
    "    cudaq.set_target(\"qpp-cpu\")\n",
    "    print(\"⚠️ GPU no encontrada. Usando CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324333d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b13cbb16",
   "metadata": {},
   "source": [
    "## Descripción del Conjunto de Datos MNIST\n",
    "\n",
    "- *¿Qué es?:* MNIST (Modified National Institute of Standards and Technology database) es una gran base de datos de dígitos escritos a mano, del 0 al 9.\n",
    "- *Contenido:* Contiene 70,000 imágenes en escala de grises.\n",
    "- *Conjunto de entrenamiento:* 60,000 imágenes.\n",
    "- *Conjunto de prueba:* 10,000 imágenes.\n",
    "- *Formato de imagen:* Cada imagen tiene un tamaño de 28x28 píxeles.\n",
    "- *Uso común:* Es considerado el \"Hola, Mundo\" de la visión por computadora y el aprendizaje profundo. Se utiliza para entrenar y probar algoritmos de clasificación de imágenes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeaf4a4",
   "metadata": {},
   "source": [
    "Paso 2: Cargar, Transformar y Previsualizar los Datos\n",
    "torchvision nos facilita la descarga y preparación de datasets.\n",
    "\n",
    "Transformaciones: Convertimos las imágenes a tensores de PyTorch y las normalizamos. La normalización (ajustar los valores de los píxeles para que tengan una media de 0.5 y una desviación estándar de 0.5) ayuda a que el modelo entrene más rápido y de forma más estable.\n",
    "Descarga: Descargamos los conjuntos de entrenamiento y prueba. FashionMNIST ya viene separado en train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e44f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación 4x4 inspirada en la reducción del paper (Figura 2)\n",
    "# 1) Normaliza a [0,1] con ToTensor\n",
    "# 2) Recorta 4 píxeles por lado (28 -> 20)\n",
    "# 3) Promedia cada bloque 5x5 para obtener una imagen 4x4\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: t[:, 4:-4, 4:-4]),\n",
    "    transforms.Lambda(lambda t: F.avg_pool2d(t, kernel_size=5, stride=5)),\n",
    "])\n",
    "\n",
    "# Descargar los datasets de entrenamiento y prueba con la transformación reducida\n",
    "train_full_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Tamaño total del dataset de entrenamiento: {len(train_full_dataset)}\")\n",
    "print(f\"Tamaño del dataset de prueba: {len(test_dataset)}\")\n",
    "print(f\"Dimensión tras reducción: {train_full_dataset[0][0].shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subconjunto configurable (por defecto se usa todo el dataset de entrenamiento reducido)\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "train_subset_for_testing = Subset(train_full_dataset, range(n_samples_prueba))\n",
    "\n",
    "train_size = int(0.8 * len(train_subset_for_testing))\n",
    "val_size = len(train_subset_for_testing) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_subset_for_testing, [train_size, val_size])\n",
    "\n",
    "print(f\"Tamaño del subconjunto de entrenamiento: {len(train_dataset)}\")\n",
    "print(f\"Tamaño del subconjunto de validación: {len(val_dataset)}\")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clases de MNIST (dígitos del 0 al 9)\n",
    "classes = tuple(str(i) for i in range(10))\n",
    "\n",
    "# Función para mostrar imágenes reducidas\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.squeeze(np.transpose(npimg, (1, 2, 0))), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Obtener un lote de imágenes de entrenamiento\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Mostrar las primeras 8 imágenes reducidas\n",
    "imshow(torchvision.utils.make_grid(images[:8], nrow=8))\n",
    "print('Etiquetas: ', ' '.join(f'{classes[labels[j]]}' for j in range(8)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffdd3f5",
   "metadata": {},
   "source": [
    "### Nota sobre hiperparámetros\n",
    "Los hiperparámetros (incluido `n_samples_prueba`) se definen una sola vez en la celda superior. Ajusta ahí el valor (ej. 2000) y ejecuta en orden para que el subconjunto use ese número; no hay redefinición posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparameters-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hiperparámetros del Modelo y Entrenamiento ---\n",
    "# Versión ligera: 4 qubits + medición AMM (12 features) + PQC en escalera poco profunda\n",
    "n_qubits = 4\n",
    "pqc_layers = 2  # profundidad baja para reducir llamadas cuánticas\n",
    "n_samples_prueba = 12000  # subconjunto para experimentar rápido; pon 60000 para todo MNIST reducido\n",
    "batch_size = 64\n",
    "epochs = 15\n",
    "learning_rate = 8e-4  # LR moderado para estabilidad con el bloque cuántico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd9cb5-5e94-40f2-9ede-5a9ce6e0bc0c",
   "metadata": {},
   "source": [
    "## Implementación del Modelo Híbrido Cuántico-Clásico\n",
    "\n",
    "Ahora, construiremos y entrenaremos el modelo híbrido. Este modelo combinará capas convolucionales clásicas para la extracción de características con un circuito cuántico parametrizado para el procesamiento de la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c47104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "from cudaq import spin\n",
    "\n",
    "# Circuito AMM con pocos qubits y medición X/Y/Z en todos los qubits\n",
    "ladder_depth = pqc_layers\n",
    "theta_kernel, data_params, theta_params = cudaq.make_kernel(list, list)\n",
    "qubits = theta_kernel.qalloc(n_qubits)\n",
    "\n",
    "# Codificación por ángulos RY\n",
    "for i in range(n_qubits):\n",
    "    theta_kernel.ry(data_params[i], qubits[i])\n",
    "\n",
    "# PQC en escalera con compuertas ZZ descompuestas en CX-RZ-CX\n",
    "theta_index = 0\n",
    "for _ in range(ladder_depth):\n",
    "    for j in range(n_qubits - 1):\n",
    "        theta_kernel.cx(qubits[j], qubits[j + 1])\n",
    "        theta_kernel.rz(2 * theta_params[theta_index], qubits[j + 1])\n",
    "        theta_kernel.cx(qubits[j], qubits[j + 1])\n",
    "        theta_index += 1\n",
    "\n",
    "# Medición X/Y/Z en todos los qubits -> 3 * n_qubits salidas\n",
    "measurement_ops = []\n",
    "for op in (spin.x, spin.y, spin.z):\n",
    "    for q in range(n_qubits):\n",
    "        measurement_ops.append(op(q))\n",
    "\n",
    "\n",
    "def evaluate_expectations(sample_angles, theta_values):\n",
    "    \"\"\"Ejecuta el kernel y devuelve <X|Y|Z> de cada qubit.\"\"\"\n",
    "    obs_results = []\n",
    "    for obs in measurement_ops:\n",
    "        res = cudaq.observe(theta_kernel, obs, sample_angles, theta_values)\n",
    "        obs_results.append(res.expectation())\n",
    "    return obs_results\n",
    "\n",
    "\n",
    "def evaluate_batch(data_tensor, theta_values, device):\n",
    "    \"\"\"Evalúa un batch completo para usar en forward y backward.\"\"\"\n",
    "    batch_results = []\n",
    "    for row in data_tensor:\n",
    "        batch_results.append(evaluate_expectations(row.detach().cpu().tolist(), theta_values))\n",
    "    return torch.tensor(batch_results, device=device, dtype=data_tensor.dtype)\n",
    "\n",
    "\n",
    "class QuantumFunction(Function):\n",
    "    \"\"\"Autograd manual: gradiente solo sobre parámetros cuánticos para ahorrar cómputo.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, data: torch.Tensor, thetas: torch.Tensor):\n",
    "        ctx.save_for_backward(data, thetas)\n",
    "        theta_list = thetas.detach().cpu().tolist()\n",
    "        return evaluate_batch(data, theta_list, data.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        data, thetas = ctx.saved_tensors\n",
    "        theta_list = thetas.detach().cpu().tolist()\n",
    "        shift = np.pi / 2.0\n",
    "\n",
    "        gradients_theta = torch.zeros_like(thetas)\n",
    "        # Solo calculamos gradientes respecto a theta (la entrada no requiere gradiente)\n",
    "        for t_idx in range(len(theta_list)):\n",
    "            theta_plus = theta_list.copy()\n",
    "            theta_minus = theta_list.copy()\n",
    "            theta_plus[t_idx] += shift\n",
    "            theta_minus[t_idx] -= shift\n",
    "\n",
    "            exp_plus = evaluate_batch(data, theta_plus, data.device)\n",
    "            exp_minus = evaluate_batch(data, theta_minus, data.device)\n",
    "            gradient_component = 0.5 * (exp_plus - exp_minus)\n",
    "            gradients_theta[t_idx] = (gradient_component * grad_output).sum()\n",
    "\n",
    "        return None, gradients_theta\n",
    "\n",
    "\n",
    "class QuantumLayer(nn.Module):\n",
    "    \"\"\"Capa cuántica con parámetros entrenables del PQC (profundidad en escalera).\"\"\"\n",
    "    def __init__(self, n_qubits: int, ladder_depth: int):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.ladder_depth = ladder_depth\n",
    "        self.theta = nn.Parameter(torch.zeros((n_qubits - 1) * ladder_depth))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return QuantumFunction.apply(x, self.theta)\n",
    "\n",
    "\n",
    "class HybridAMM(nn.Module):\n",
    "    \"\"\"Modelo híbrido ligero: encoder clásico pequeño + PQC AMM + clasificador.\"\"\"\n",
    "    def __init__(self, n_qubits: int = n_qubits, ladder_depth: int = pqc_layers):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "\n",
    "        # Encoder clásico compacto para comprimir 4x4=16 features a 4 ángulos\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_qubits),\n",
    "        )\n",
    "\n",
    "        self.quantum = QuantumLayer(n_qubits, ladder_depth)\n",
    "\n",
    "        # 3 observables por qubit -> 12 features para 4 qubits\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(3 * n_qubits),\n",
    "            nn.Linear(3 * n_qubits, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normaliza a [0,1] y usa arcsin como en el paper\n",
    "        angles = torch.sigmoid(self.encoder(x))\n",
    "        angles = torch.arcsin(torch.clamp(angles, 0.0, 1.0))\n",
    "\n",
    "        q_out = self.quantum(angles)\n",
    "        logits = self.classifier(q_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba130a2-392d-4f50-bb18-852413f7bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Creación y Validación del Modelo ---\n",
    "\n",
    "# Crear instancia del modelo híbrido ligero con AMM\n",
    "hybrid_model = HybridAMM(n_qubits=n_qubits, ladder_depth=pqc_layers).to(device)\n",
    "print(\"--- Arquitectura del Modelo Híbrido (AMM ligero) ---\")\n",
    "print(hybrid_model)\n",
    "\n",
    "# --- Test de Validación Cuántico-Clásico (Forward y Backward) ---\n",
    "print(\"--- Realizando Test de Validación Cuántico-Clásico ---\")\n",
    "\n",
    "test_input = torch.randn(4, 1, 4, 4).to(device)  # imágenes ya reducidas a 4x4\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "output = hybrid_model(test_input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"✅ Pase hacia adelante completado\")\n",
    "\n",
    "# Pase hacia atrás\n",
    "target = torch.randint(0, 10, (4,)).to(device)\n",
    "loss_fn_test = nn.CrossEntropyLoss()\n",
    "loss = loss_fn_test(output, target)\n",
    "loss.backward()\n",
    "\n",
    "grad_pre_quantum = list(hybrid_model.quantum.parameters())[0].grad\n",
    "if grad_pre_quantum is not None and grad_pre_quantum.abs().sum() > 0:\n",
    "    print(\"✅ Backward funcionando: gradientes fluyen hasta el PQC y el clasificador.\")\n",
    "else:\n",
    "    print(\"❌ Error en el backward: sin gradientes en el PQC.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-train-markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del Modelo Híbrido\n",
    "\n",
    "Ahora, entrenaremos el modelo híbrido usando el mismo bucle de entrenamiento que para el modelo clásico. Notarás que el entrenamiento puede ser más lento debido a la simulación de los circuitos cuánticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-train-validation-loops",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definir los bucles de entrenamiento y validación ---\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"Bucle para una época de entrenamiento.\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() # Poner el modelo en modo de entrenamiento\n",
    "    total_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Mover datos al dispositivo (CPU o GPU)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Calcular la predicción y la pérdida\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validation_loop(dataloader, model, loss_fn):\n",
    "    \"\"\"Bucle para evaluar el modelo en el conjunto de validación o prueba.\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() # Poner el modelo en modo de evaluación\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad(): # No necesitamos calcular gradientes aquí\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-train-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Entrenamiento del Modelo Híbrido ---\n",
    "import time\n",
    "# Definir la función de pérdida y el optimizador\n",
    "loss_fn_hybrid = nn.CrossEntropyLoss()\n",
    "optimizer_hybrid = optim.Adam(hybrid_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Reducimos el learning rate en un factor de 0.2 cada 7 épocas\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer_hybrid, step_size=7, gamma=0.2)\n",
    "# Listas para almacenar el historial de entrenamiento\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "# Bucle principal de entrenamiento para el modelo híbrido\n",
    "print(\"\n",
    "--- Iniciando Entrenamiento del Modelo Híbrido AMM con MNIST ---\")\n",
    "start_time = time.time()\n",
    "for t in range(epochs):\n",
    "    print(f\"Época {t+1}\n",
    "-------------------------------\")\n",
    "    print(\"Entrenando (Híbrido AMM)...\")\n",
    "    train_loss = train_loop(train_loader, hybrid_model, loss_fn_hybrid, optimizer_hybrid)\n",
    "    print(\"Validando (Híbrido AMM)...\")\n",
    "    val_loss, val_acc = validation_loop(val_loader, hybrid_model, loss_fn_hybrid)\n",
    "    \n",
    "    # Guardar métricas de la época\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_accuracy'].append(val_acc)\n",
    "    scheduler.step()\n",
    "\n",
    "end_time = time.time()\n",
    "total_training_time = end_time - start_time\n",
    "print(\"¡Entrenamiento híbrido finalizado!\")\n",
    "\n",
    "# Evaluar el modelo híbrido en el conjunto de prueba\n",
    "print(\"\n",
    "--- Evaluando Modelo Híbrido AMM con el conjunto de Prueba (Test) ---\")\n",
    "validation_loop(test_loader, hybrid_model, loss_fn_hybrid)\n",
    "\n",
    "# Guardar los pesos del modelo híbrido entrenado\n",
    "save_model(hybrid_model, \"hybrid_amm_mnist_weights_gpu\" if device==\"cuda\" else \"hybrid_amm_mnist_weights_cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualización de Resultados ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Usamos las variables definidas en las celdas anteriores\n",
    "num_epochs = epochs\n",
    "num_train_samples = len(train_dataset)\n",
    "\n",
    "# Determinar el nombre del dispositivo para el título de la gráfica\n",
    "if device == \"cuda\":\n",
    "    device_name = f\"GPU ({torch.cuda.get_device_name(0)})\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Título principal de la figura con la nueva información del dispositivo\n",
    "title_text = \"\n",
    "\".join([\n",
    "    \"Modelo Híbrido AMM - Historial de Entrenamiento\",\n",
    "    f\"({num_epochs} épocas en {num_train_samples} muestras) batch_size={batch_size}\",\n",
    "    f\"Ejecutado en: {device_name}\",\n",
    "    f\"Tiempo total de entrenamiento: {total_training_time:.2f} segundos\",\n",
    "])\n",
    "fig.suptitle(title_text, fontsize=16)\n",
    "\n",
    "# Gráfica de Pérdida (Loss)\n",
    "ax1.plot(history['train_loss'], label='Pérdida de Entrenamiento', marker='o')\n",
    "ax1.plot(history['val_loss'], label='Pérdida de Validación', marker='o')\n",
    "ax1.set_ylabel('Pérdida (Loss)')\n",
    "ax1.set_title('Pérdida a lo largo de las Épocas')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Gráfica de Precisión (Accuracy)\n",
    "ax2.plot(history['val_accuracy'], label='Precisión de Validación', color='green', marker='o')\n",
    "ax2.set_xlabel('Épocas')\n",
    "ax2.set_ylabel('Precisión (Accuracy)')\n",
    "ax2.set_title('Precisión de Validación a lo largo de las Épocas')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-random-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "hybrid_model.eval()\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "idx = random.randint(0, len(images) - 1)\n",
    "img = images[idx].unsqueeze(0)\n",
    "true_label = labels[idx]\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = hybrid_model(img)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "img_display = img.cpu().squeeze()\n",
    "plt.imshow(img_display, cmap='gray')\n",
    "plt.title(f\"Etiqueta Real: {classes[true_label]} | Predicción: {classes[predicted.item()]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4e51f-bc3c-4bdf-bfa7-303685cb3d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132a6f4-2866-4e21-b4fb-6b0c0bced3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_braket",
   "language": "python",
   "name": "conda_braket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
